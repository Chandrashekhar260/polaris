backend/
â”œâ”€â”€ main.py
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ stream.py
â”‚   â”œâ”€â”€ insights.py
â”‚   â”œâ”€â”€ recommendations.py
â”‚   â”œâ”€â”€ summary.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ai_agent.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ recommender.py
â”‚   â”œâ”€â”€ summarizer.py
â”‚   â”œâ”€â”€ websocket_manager.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ env_loader.py
â”‚   â”œâ”€â”€ text_cleaner.py
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ chroma_store/
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md

---

### ğŸ”Œ API & WEBSOCKET DETAILS

#### `/ws/stream` (WebSocket)
- Receives live code or text updates from local client.
- Backend immediately:
  - Sends text to Gemini (via LangChain).
  - Extracts topic, concepts, and confusion points.
  - Generates embeddings and stores in ChromaDB.
  - Returns real-time AI insights back to the client socket.

#### `/insights` [GET]
- Returns the latest Gemini-based analysis, key mistakes, and topic difficulty.

#### `/recommendations` [GET]
- Returns learning materials (docs, tutorials, or explanations) relevant to the userâ€™s recent struggles.

#### `/summary` [GET]
- Returns daily or weekly AI-generated learning summary using stored data.

---

### âš™ï¸ ENVIRONMENT VARIABLES (.env.example)

GOOGLE_API_KEY=your_gemini_api_key_here
CHROMA_DB_PATH=./db/chroma_store
BACKEND_PORT=8000
ğŸ’¡ Get Gemini API key here: https://makersuite.google.com/app/apikey

---

### ğŸ“˜ REQUIREMENTS.TXT

fastapi
uvicorn
langchain
chromadb
google-generativeai
requests
python-dotenv
websockets

---

### âš™ï¸ WORKFLOW
1. Local **watcher client** in VS Code detects file changes and streams data via WebSocket to backend `/ws/stream`.  
2. Backend (Replit) receives data â†’ cleans it â†’ sends to Gemini â†’ gets insights & topics â†’ stores embeddings in ChromaDB.  
3. When user queries `/insights`, `/recommendations`, or `/summary`, backend fetches data + generates responses via Gemini.  
4. All updates happen automatically in real time â€” **no manual upload needed**.  

---

### ğŸ§© CONDITIONS
- Must use **Gemini API** for all AI operations (summarization, recommendations, difficulty detection).  
- Must store embeddings locally (ChromaDB).  
- Must support **real-time processing** via WebSocket or SSE.  
- Must create a clear modular structure with all services separated.  
- Must include `.env.example`, `requirements.txt`, and `README.md`.

---

### âœ… EXPECTED OUTPUT
Generate a **complete backend folder** that:
- Runs with `uvicorn main:app --reload`  
- Supports live WebSocket input from VS Code watcher  
- Uses Gemini + LangChain for AI analysis  
- Stores all embeddings locally in ChromaDB  
- Provides REST APIs for insights, recommendations, and summaries  
- Includes all required setup files for Replit  


Would you like me to now give the next prompt for your VS Code watcher client â€” which detects file changes and sends them live to this backend?